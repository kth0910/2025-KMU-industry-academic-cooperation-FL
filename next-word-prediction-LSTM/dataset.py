# dataset.py (streaming + sharding, memory-safe)
import os
import sys
import json
import gzip
import pickle
import math
from collections import Counter, deque, defaultdict
from tqdm import tqdm

# ===== Config =====
INPUT_FILE = "reddit.jsonl"        # ì›ë³¸ JSONL (fields: author|user_id, body)
OUTPUT_DIR = "data"                # ì¶œë ¥ ë””ë ‰í† ë¦¬ ë£¨íŠ¸
VOCAB_SIZE = 10000                 # <UNK> í¬í•¨ 10k (ì‹¤ì œ top-9999 + <UNK>)
MAX_WORDS_PER_CLIENT = 5000        # í´ë¼ì´ì–¸íŠ¸ë‹¹ ë‹¨ì–´ ìƒí•œ
UNROLL_LENGTH = 10                 # LSTM unroll
TEST_POSTS = 100000                # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ê°œìˆ˜ ëª©í‘œ
SHARD_MAX_LINES = 500_000          # í•™ìŠµ ìƒ˜í”Œ ìƒ¤ë“œë‹¹ ìµœëŒ€ ë¼ì¸ ìˆ˜
SHARD_DIR = "train_shards"         # í•™ìŠµ ìƒ¤ë“œ í´ë”

UNK = "<UNK>"

def _ensure_dirs():
    if not os.path.isfile(INPUT_FILE):
        sys.exit(f"[Error] Reddit JSONL input file not found: {INPUT_FILE}")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(os.path.join(OUTPUT_DIR, SHARD_DIR), exist_ok=True)
    
# dataset.py ë§¨ ìœ„ìª½ì— ì¶”ê°€
def validate_dataset():
    """ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ì™€ ì›ë³¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ê²€ì¦."""
    _ensure_dirs()

def build_vocab():
    """
    Pass 1: ì „ì²´ ì½”í¼ìŠ¤ 1íšŒ ìŠ¤ìº” â†’ ì „ì—­ ë‹¨ì–´ ë¹ˆë„ Counter â†’ vocab.pkl ìƒì„±
    ë©”ëª¨ë¦¬: Counterë§Œ ìœ ì§€
    """
    counter = Counter()
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="ğŸ”¢ Pass1: Vocab counting"):
            try:
                post = json.loads(line)
            except json.JSONDecodeError:
                continue
            text = (post.get("body") or "").strip()
            if not text:
                continue
            words = text.split()
            counter.update(words)

    # top-(VOCAB_SIZE-1) + UNK=0
    most_common = [w for w, _ in counter.most_common(VOCAB_SIZE - 1)]
    vocab = {UNK: 0}
    for i, w in enumerate(most_common, start=1):
        vocab[w] = i

    with open(os.path.join(OUTPUT_DIR, "vocab.pkl"), "wb") as f:
        pickle.dump(vocab, f)
    return vocab

def _open_new_shard(shard_idx):
    path = os.path.join(OUTPUT_DIR, SHARD_DIR, f"train_{shard_idx:05d}.tsv.gz")
    fh = gzip.open(path, "wt", encoding="utf-8")
    return fh, path

def generate_train_shards(vocab):
    """
    Pass 2: ë‹¤ì‹œ ìŠ¤ìº” â†’ per-user deque(ê¸¸ì´=UNROLL_LENGTH)ì™€ ì‚¬ìš© ë‹¨ì–´ ìˆ˜ë§Œ ìœ ì§€
            ìœˆë„ìš°ê°€ ì°° ë•Œë§ˆë‹¤ ì¦‰ì‹œ ë¼ì¸ìœ¼ë¡œ ê¸°ë¡: user \t "id1 id2 ... id10" \t tgt
    ì¶œë ¥: data/train_shards/*.tsv.gz, data/manifest.json
    """
    # per-user ìƒíƒœ(ê°€ë²¼ì›€)
    buffers = defaultdict(lambda: deque(maxlen=UNROLL_LENGTH))  # ìµœê·¼ UNROLL_LENGTH í† í°
    used_counts = defaultdict(int)                              # ìœ ì €ë³„ ì´ë¯¸ ì‚¬ìš©í•œ ë‹¨ì–´ ìˆ˜ (<= 5000)
    train_users = set()                                         # í•™ìŠµì— 1ê°œ ì´ìƒ ë°°ì¶œí•œ ìœ ì €

    shard_idx = 1
    shard_lines = 0
    total_lines = 0
    shard_fh, shard_path = _open_new_shard(shard_idx)
    shard_paths = [shard_path]

    def flush_if_needed():
        nonlocal shard_idx, shard_lines, shard_fh, shard_paths
        if shard_lines >= SHARD_MAX_LINES:
            shard_fh.close()
            shard_idx += 1
            shard_fh, path = _open_new_shard(shard_idx)
            shard_paths.append(path)
            shard_lines = 0

    unk_id = vocab.get(UNK, 0)

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="ğŸ§  Pass2: Train shards"):
            try:
                post = json.loads(line)
            except json.JSONDecodeError:
                continue
            user = post.get("author") or post.get("user_id") or "<UNK_USER>"
            text = (post.get("body") or "").strip()
            if not text:
                continue
            if used_counts[user] >= MAX_WORDS_PER_CLIENT:
                continue

            # ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œ ì“¸ ìˆ˜ ìˆëŠ” ë‚¨ì€ í† í° ìˆ˜
            remaining = MAX_WORDS_PER_CLIENT - used_counts[user]
            tokens = text.split()
            if not tokens:
                continue
            if len(tokens) > remaining:
                tokens = tokens[:remaining]

            # í† í° â†’ ids
            for w in tokens:
                wid = vocab.get(w, unk_id)
                buf = buffers[user]
                if len(buf) == UNROLL_LENGTH:
                    # ìƒ˜í”Œ ë°°ì¶œ
                    seq_ids = " ".join(str(x) for x in buf)
                    shard_fh.write(f"{user}\t{seq_ids}\t{wid}\n")
                    shard_lines += 1
                    total_lines += 1
                    train_users.add(user)
                    flush_if_needed()
                buf.append(wid)
                used_counts[user] += 1

    shard_fh.close()

    # manifest ì €ì¥
    manifest = {
        "train_shards": shard_paths,
        "num_train_samples": total_lines,
        "num_train_users": len(train_users),
        "unroll_length": UNROLL_LENGTH,
        "vocab_size": len(vocab),
        "max_words_per_client": MAX_WORDS_PER_CLIENT,
    }
    with open(os.path.join(OUTPUT_DIR, "manifest.json"), "w", encoding="utf-8") as f:
        json.dump(manifest, f, ensure_ascii=False, indent=2)

    return train_users, total_lines, shard_paths

def build_test_set(vocab, train_users):
    """
    Pass 3: í•™ìŠµì— í•œ ë²ˆë„ ì“°ì´ì§€ ì•Šì€ ì‚¬ìš©ìì—ì„œ TEST_POSTS ìƒ˜í”Œ ìˆ˜ì§‘
            ê°„ë‹¨íˆ ê° í¬ìŠ¤íŠ¸ì˜ ì• UNROLL_LENGTH+1 í† í°ë§Œ ì‚¬ìš©
    ì¶œë ¥: data/test.tsv.gz (user \t "id1..id10" \t tgt)
    """
    out_path = os.path.join(OUTPUT_DIR, "test.tsv.gz")
    written = 0
    unk_id = vocab.get(UNK, 0)

    with gzip.open(out_path, "wt", encoding="utf-8") as out_f, \
         open(INPUT_FILE, "r", encoding="utf-8") as f:
        for line in tqdm(f, desc="ğŸ§ª Pass3: Test set"):
            if written >= TEST_POSTS:
                break
            try:
                post = json.loads(line)
            except json.JSONDecodeError:
                continue
            user = post.get("author") or post.get("user_id") or "<UNK_USER>"
            if user in train_users:
                continue
            words = (post.get("body") or "").split()
            if len(words) <= UNROLL_LENGTH:
                continue
            seq = words[:UNROLL_LENGTH]
            tgt = words[UNROLL_LENGTH]
            seq_ids = " ".join(str(vocab.get(w, unk_id)) for w in seq)
            tgt_id = vocab.get(tgt, unk_id)
            out_f.write(f"{user}\t{seq_ids}\t{tgt_id}\n")
            written += 1

    return out_path, written

def build_dataset():
    _ensure_dirs()
    vocab = build_vocab()
    train_users, n_samples, shard_paths = generate_train_shards(vocab)
    test_path, n_test = build_test_set(vocab, train_users)

    # ìš”ì•½ ë¡œê·¸
    print("\nâœ… Done.")
    print(f"- Vocab: {len(vocab):,} (saved to {os.path.join(OUTPUT_DIR,'vocab.pkl')})")
    print(f"- Train samples: {n_samples:,} across {len(shard_paths)} shard(s)")
    print(f"- Test samples:  {n_test:,} (file: {test_path})")
    print(f"- Manifest:      {os.path.join(OUTPUT_DIR,'manifest.json')}")

# -------- Optional: simple loaders --------
def iter_train_samples(data_dir="data"):
    """
    ìƒ¤ë“œ íŒŒì¼ë“¤ì„ ë¼ì¸ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ (user, seq_ids[list[int]], tgt_id[int]) ìƒì„±
    """
    manifest_path = os.path.join(data_dir, "manifest.json")
    with open(manifest_path, "r", encoding="utf-8") as f:
        mani = json.load(f)
    for p in mani["train_shards"]:
        with gzip.open(p, "rt", encoding="utf-8") as fh:
            for line in fh:
                user, seq_str, tgt_str = line.rstrip("\n").split("\t")
                seq = [int(x) for x in seq_str.split()]
                tgt = int(tgt_str)
                yield user, seq, tgt

def iter_test_samples(data_dir="data"):
    """
    test.tsv.gzë¥¼ ë¼ì¸ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•´ì„œ (user, seq_ids[list[int]], tgt_id[int]) ìƒì„±
    """
    path = os.path.join(data_dir, "test.tsv.gz")
    with gzip.open(path, "rt", encoding="utf-8") as fh:
        for line in fh:
            user, seq_str, tgt_str = line.rstrip("\n").split("\t")
            seq = [int(x) for x in seq_str.split()]
            tgt = int(tgt_str)
            yield user, seq, tgt

if __name__ == "__main__":
    build_dataset()
